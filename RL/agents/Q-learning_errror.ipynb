{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    " \n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "from utilities.buffer import ReplayMemory,Transition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    " \n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n",
    "from utilities.matrix_square_root import sqrtm\n",
    "\n",
    "def QP_layer_no_eq(nz, nineq_u, nineq_x):\n",
    "    \"\"\"Builds the QP layer without equality constraints\n",
    "    The optimization problem is of the form\n",
    "        \\hat z,\\hat e  =   argmin_z z^T*Q*z + p^T*z + e^T*E*e\n",
    "                subject to G1*z <= h1\n",
    "                            G2*z <= h2+e\n",
    "    \"\"\"\n",
    "    Q_sqrt = cp.Parameter((nz, nz))\n",
    "    p = cp.Parameter(nz)\n",
    "    G1 = cp.Parameter((nineq_u, nz))\n",
    "    h1 = cp.Parameter(nineq_u)\n",
    "    G2 = cp.Parameter((nineq_x, nz))\n",
    "    h2 = cp.Parameter(nineq_x)\n",
    "    E_sqrt = cp.Parameter((nineq_x, nineq_x))\n",
    "    z = cp.Variable(nz)\n",
    "    e = cp.Variable(nineq_x)\n",
    "    zero =cp.Parameter(nineq_x) \n",
    "    \n",
    "    obj = cp.Minimize(cp.sum_squares(Q_sqrt*z) + p.T@z +\n",
    "                     cp.sum_squares(E_sqrt*e))\n",
    "    cons = [ G1@z <= h1,G2@z <= h2+e , e >= zero ]# , e >= 0\n",
    "    prob = cp. Problem(obj, cons)\n",
    "    assert prob.is_dpp()\n",
    "\n",
    "    layer = CvxpyLayer (prob, \n",
    "                        parameters =[Q_sqrt, p, G1, h1, G2,\n",
    "                                     h2, E_sqrt,zero], \n",
    "                        variables =[ z, e ])\n",
    "    return layer\n",
    "\n",
    "def QP_layer_no_eq_e(nz, nineq_u, nineq_x):\n",
    "    \"\"\"Builds the QP layer without equality constraints\n",
    "    The optimization problem is of the form\n",
    "        \\hat z,\\hat e  =   argmin_z z^T*Q*z + p^T*z + e^T*E*e\n",
    "                subject to G1*z <= h1\n",
    "                            G2*z <= h2+e\n",
    "    \"\"\"\n",
    "    Q_sqrt = cp.Parameter((nz, nz))\n",
    "    p = cp.Parameter(nz)\n",
    "    G1 = cp.Parameter((nineq_u, nz))\n",
    "    h1 = cp.Parameter(nineq_u)\n",
    "    G2 = cp.Parameter((nineq_x, nz))\n",
    "    h2 = cp.Parameter(nineq_x)\n",
    "    E_sqrt = cp.Parameter((nineq_x, nineq_x))\n",
    "    z = cp.Variable(nz)\n",
    "    e = cp.Variable(nineq_x)\n",
    "    obj = cp.Minimize(cp.sum_squares(Q_sqrt*z) + p.T@z +\n",
    "                     cp.sum_squares(E_sqrt*e))\n",
    "    cons = [ G1@z <= h1,G2@z <= h2+e ]# , e >= 0\n",
    "    prob = cp. Problem(obj, cons)\n",
    "    assert prob.is_dpp()\n",
    "\n",
    "    layer = CvxpyLayer (prob, \n",
    "                        parameters =[Q_sqrt, p, G1, h1, G2,\n",
    "                                     h2, E_sqrt], \n",
    "                        variables =[ z, e ])\n",
    "    return layer\n",
    "\n",
    "def QP_layer(nz, nineq_u, nineq_x, neq):\n",
    "    \"\"\"Builds the QP layer with MPC soft constraints.\n",
    "\n",
    "    The optimization problem is of the form\n",
    "        \\hat z,\\hat e  =   argmin_z z^T*Q*z + p^T*z + e^T*E*e\n",
    "                subject to G1*z <= h1\n",
    "                           G2*z <= h2+e\n",
    "                           A*z = b\n",
    "                \n",
    "    where Q \\in S^{nz,nz},\n",
    "        S^{nz,nz} is the set of all positive semi-definite matrices,\n",
    "        p \\in R^{nz}\n",
    "        G1 \\in R^{nineq_u,nz}\n",
    "        h1 \\in R^{nineq_u}\n",
    "        G2 \\in R^{nineq_x,nz}\n",
    "        h2 \\in R^{nineq_x}\n",
    "        A \\in R^{neq,nz}\n",
    "        b \\in R^{neq}\n",
    "        E \\in S^{ne,ne}, where ne = nineq_x\n",
    "    \n",
    "    Take the matrix square-root of Q：mentioned in paper P19\n",
    "    (Differentiable Convex Optimization Layers).\n",
    "    \"\"\"\n",
    "    Q_sqrt = cp.Parameter((nz, nz))\n",
    "    p = cp.Parameter(nz)\n",
    "    G1 = cp.Parameter((nineq_u, nz))\n",
    "    h1 = cp.Parameter(nineq_u)\n",
    "    G2 = cp.Parameter((nineq_x, nz))\n",
    "    h2 = cp.Parameter(nineq_x)\n",
    "    A = cp.Parameter ((neq,nz))\n",
    "    b = cp.Parameter (neq)\n",
    "    E_sqrt = cp.Parameter((nineq_x, nineq_x))\n",
    "    z = cp.Variable(nz)\n",
    "    e = cp.Variable(nineq_x)\n",
    "    zero =cp.Parameter(nineq_x)\n",
    "    \n",
    "    obj = cp.Minimize(cp.sum_squares(Q_sqrt*z) + p.T@z +\n",
    "                     cp.sum_squares(E_sqrt*e))\n",
    "    cons = [ G1@z <= h1,G2@z <= h2+e, A@z == b, e >= zero ]#, e >= 0\n",
    "    prob = cp. Problem(obj, cons)\n",
    "    assert prob.is_dpp()\n",
    "\n",
    "    layer = CvxpyLayer (prob, \n",
    "                        parameters =[Q_sqrt, p, G1, h1, G2,\n",
    "                                     h2, E_sqrt, A, b,zero], \n",
    "                        variables =[ z, e ])\n",
    "    return layer\n",
    "def QP_layer_e(nz, nineq_u, nineq_x, neq):\n",
    "    \"\"\"Builds the QP layer with MPC soft constraints.\n",
    "\n",
    "    The optimization problem is of the form\n",
    "        \\hat z,\\hat e  =   argmin_z z^T*Q*z + p^T*z + e^T*E*e\n",
    "                subject to G1*z <= h1\n",
    "                           G2*z <= h2+e\n",
    "                           A*z = b\n",
    "                \n",
    "    where Q \\in S^{nz,nz},\n",
    "        S^{nz,nz} is the set of all positive semi-definite matrices,\n",
    "        p \\in R^{nz}\n",
    "        G1 \\in R^{nineq_u,nz}\n",
    "        h1 \\in R^{nineq_u}\n",
    "        G2 \\in R^{nineq_x,nz}\n",
    "        h2 \\in R^{nineq_x}\n",
    "        A \\in R^{neq,nz}\n",
    "        b \\in R^{neq}\n",
    "        E \\in S^{ne,ne}, where ne = nineq_x\n",
    "    \n",
    "    Take the matrix square-root of Q：mentioned in paper P19\n",
    "    (Differentiable Convex Optimization Layers).\n",
    "    \"\"\"\n",
    "    Q_sqrt = cp.Parameter((nz, nz))\n",
    "    p = cp.Parameter(nz)\n",
    "    G1 = cp.Parameter((nineq_u, nz))\n",
    "    h1 = cp.Parameter(nineq_u)\n",
    "    G2 = cp.Parameter((nineq_x, nz))\n",
    "    h2 = cp.Parameter(nineq_x)\n",
    "    A = cp.Parameter ((neq,nz))\n",
    "    b = cp.Parameter (neq)\n",
    "    E_sqrt = cp.Parameter((nineq_x, nineq_x))\n",
    "    z = cp.Variable(nz)\n",
    "    e = cp.Variable(nineq_x)\n",
    "    obj = cp.Minimize(cp.sum_squares(Q_sqrt*z) + p.T@z +\n",
    "                     cp.sum_squares(E_sqrt*e))\n",
    "    cons = [ G1@z <= h1,G2@z <= h2+e, A@z == b ]#, e >= 0\n",
    "    prob = cp. Problem(obj, cons)\n",
    "    assert prob.is_dpp()\n",
    "\n",
    "    layer = CvxpyLayer (prob, \n",
    "                        parameters =[Q_sqrt, p, G1, h1, G2,\n",
    "                                     h2, E_sqrt, A, b], \n",
    "                        variables =[ z, e ])\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cvx_Nets(nn.Module):\n",
    "    \"\"\"Builds the nets for Q function in Q-learning.\n",
    "    The struture is (x0,u0)-QP-[cost,u].\n",
    "    In addition, if input only contains x0, the nets represents\n",
    "    the value funnction under current policy, which is:\n",
    "        max(u0)Q(x0,u0)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_input, num_output, num_u=5, cuda=True,collect=False):\n",
    "\n",
    "        \"\"\"Initiates the nets.\"\"\"\n",
    "        super().__init__()\n",
    "        self.num_input = num_input  # Dimension: x0\n",
    "        self.num_output = num_output  # Dimension: u0\n",
    "        self.num_u = num_u  # Dimension: u0,u1,...,uN\n",
    "        self.cuda = cuda\n",
    "        self.collect = collect\n",
    "        \n",
    "        # gets the number of the finite steps in MPC\n",
    "        self.N = int(self.num_u/self.num_output)\n",
    "        self.num_ineq_u = 2*num_u\n",
    "        self.num_ineq_x = 2*num_input*self.N\n",
    "        \n",
    "        # For Q(x0,u0),defines the QP layer used\n",
    "        self.layer = QP_layer(nz=self.num_u, nineq_u=\n",
    "                        self.num_ineq_u, nineq_x=self.num_ineq_x,\n",
    "                        neq = num_output)\n",
    "        \n",
    "        # defines parameters in the QP layer\n",
    "        self.Q_sqrt = Parameter(torch.rand(num_input, num_input),requires_grad=True)\n",
    "        self.R_sqrt = Parameter(torch.rand(num_output,num_output),requires_grad=True)\n",
    "        \n",
    "        self.A = Parameter(torch.rand(num_input, num_input),requires_grad=True)\n",
    "        self.B = Parameter(torch.rand(num_input, num_output),requires_grad=True)\n",
    "\n",
    "        self.h1 = Parameter(0.5*torch.ones(self.num_ineq_u),requires_grad=False)\n",
    "        self.h21 = Parameter(4*torch.ones(num_input*self.N),requires_grad=False)\n",
    "        self.h22 = Parameter(4*torch.ones(num_input*self.N),requires_grad=False)\n",
    "        \n",
    "        self.E_sqrt = Parameter(torch.eye(self.num_ineq_x),requires_grad=False)\n",
    "\n",
    "        if collect==True:\n",
    "            self.Q_sqrt = Parameter(torch.eye(num_input),requires_grad=True)\n",
    "            self.R_sqrt = Parameter(torch.eye(num_output),requires_grad=True)\n",
    "            self.A = Parameter(torch.Tensor([[1.0,1.0],[0,1.0]]),requires_grad=True)\n",
    "            self.B = Parameter(torch.Tensor([[0.5],[1.0]]),requires_grad=True)\n",
    "            self.h1 = Parameter(0.5*torch.ones(self.num_ineq_u),requires_grad=False)\n",
    "            self.h21 = Parameter(4*torch.ones(num_input*self.N),requires_grad=False)\n",
    "            self.h22 = Parameter(4*torch.ones(num_input*self.N),requires_grad=False)\n",
    "                               \n",
    "        weight = torch.zeros(num_u)\n",
    "        weight[0] = 1.0\n",
    "        self.weight = Parameter(weight,requires_grad=False)\n",
    "        \n",
    "        self.F = Parameter(torch.zeros(1,self.num_u),requires_grad=False)\n",
    "        self.F[0,0] = 1.0\n",
    "        self.f = Parameter(torch.tensor([1.0]),requires_grad=False)\n",
    "        \n",
    "        self.zero = Parameter(torch.zeros(self.num_ineq_x),requires_grad=False)\n",
    "\n",
    "    def forward(self, x, u0=torch.Tensor()):\n",
    "        \"\"\"Builds the forward strucre of the QPNet.\n",
    "        Sequence: x0-QP-[cost,u].\n",
    "        QP parameters: Q_sqrt, p, G, h\n",
    "        \"\"\"\n",
    "        x =self.vari_gpu(x)\n",
    "        u0 =self.vari_gpu(u0)\n",
    "        \n",
    "        # input x0 and batch size         \n",
    "        num_batch = x.size(0)\n",
    "        x0 = x.view(num_batch, -1)\n",
    "        \n",
    "        A_hat = self.build_A_block()\n",
    "        B_hat = self.build_B_block()\n",
    "        \n",
    "        # Q_sqrt in QP\n",
    "        Q = self.Q_sqrt.mm(self.Q_sqrt.t())\n",
    "        R = self.R_sqrt.mm(self.R_sqrt.t())\n",
    "        R_diag = self.build_Rdiagnol_block(R)\n",
    "        Q_hat, Q_diag = self.build_Q_block(Q, Q, R, B_hat)\n",
    "        Q_sqrt_hat = sqrtm(Q_hat)  # computs sqrt of Q\n",
    "        Q_sqrt_hat = Q_sqrt_hat.repeat(num_batch,1,1)  # builds batch\n",
    "                \n",
    "        # p in QP  p = 2 * (Q_diag*B_hat)^T * (A_hat*x0)\n",
    "        A_x0 = A_hat.mm(x0.t()).t()  # presents[x1;x2;...;xN] size: batch * dim(x1;x2;...;xN)\n",
    "        p = 2*A_x0.mm(Q_diag.mm(B_hat))\n",
    "        \n",
    "        # G in QP\n",
    "        G1,G2 = self.build_G_block(B_hat)\n",
    "        G1 = G1.repeat(num_batch,1,1)  # builds batch\n",
    "        G2 = G2.repeat(num_batch,1,1)  # builds batch\n",
    "        \n",
    "        # h in QP\n",
    "        h1 = self.h1.repeat(num_batch,1)  # builds batch\n",
    "        h21 = self.h21.repeat(num_batch,1)  # builds batch\n",
    "        h21 -= A_x0 \n",
    "        h22 = self.h22.repeat(num_batch,1)  # builds batch\n",
    "        h22 += A_x0\n",
    "        h2 = torch.cat((h21,h22),1)\n",
    "        \n",
    "        zero = self.zero.repeat(num_batch,1)\n",
    "        \n",
    "        # E in QP\n",
    "        E = self.E_sqrt.mm(self.E_sqrt.t())\n",
    "        E_sqrt = self.E_sqrt.repeat(num_batch,1,1)\n",
    "        \n",
    "        # for Q(x0,u0), add equality constraint: u(0) = u0         \n",
    "        if u0.nelement() != 0:\n",
    "            u0 = u0.view(num_batch, -1)\n",
    "            # F*z = f\n",
    "            F = self.F\n",
    "            f = u0*self.f\n",
    "            F = F.repeat(num_batch,1,1)  # builds batch\n",
    "            #f = f.repeat(num_batch,1)  # builds batch\n",
    "#             print(Q_sqrt_hat.size(), p.size(), G1.size(),\n",
    "#                   h1.size(), G2.size(),h2.size(),\n",
    "#                   E_sqrt.size(),F.size(),f.size())\n",
    "\n",
    "            self.para = [Q_sqrt_hat, p, G1, h1, G2,\n",
    "                                h2, E_sqrt, F, f]\n",
    "            u_opt,e_opt, = self.layer(Q_sqrt_hat, p, G1, h1, G2,\n",
    "                                h2, E_sqrt, F, f,zero)  # u_opt: batch*dim(u)\n",
    "        # for V(x0), defines the QP layer without equality \n",
    "        # constraints        \n",
    "        else:\n",
    "            layer = QP_layer_no_eq(nz=self.num_u, nineq_u=\n",
    "                        self.num_ineq_u, nineq_x=self.num_ineq_x)\n",
    "            self.para = [Q_sqrt_hat, p, G1, h1, G2,\n",
    "                                h2, E_sqrt]            \n",
    "            # gets the solution of the basic optimization problem\n",
    "            u_opt,e_opt, = layer(Q_sqrt_hat, p, G1, h1, G2, h2, \n",
    "                                E_sqrt,zero)  # u_opt: batch*dim(u)\n",
    "\n",
    "        # get the optimal cost\n",
    "        # a+b: sum(i:1 to N): xi^T*Q*xi + u(i-1)^T*R*u(i-1)\n",
    "        # c: x0^T*Q*x0\n",
    "        # d:(i:1 to N):ei^T*E*ei\n",
    "        a = (u_opt.mm(Q_hat)*u_opt + p*u_opt).sum(1)\n",
    "        b = (A_x0.mm(Q_diag)*A_x0).sum(1)\n",
    "        c = (x0.mm(Q)*x0).sum(1)\n",
    "        d = (e_opt.mm(E)*e_opt).sum(1)\n",
    "        cost_opt = (a+b+c+d).unsqueeze(1)  # size: batch*1\n",
    "        u0_opt = u_opt.mv(self.weight)  # only the fisrt action\n",
    "        #print(u0,u0_opt)\n",
    "        return cost_opt, u0_opt\n",
    "    \n",
    "    def build_A_block(self):\n",
    "        \"\"\"\n",
    "        [A]\n",
    "        [A^2] \n",
    "        [A^3]\n",
    "        [...]\n",
    "        \"\"\"\n",
    "        N = self.N  # number of MPC steps\n",
    "        A = self.A\n",
    "        \n",
    "        row_list = [A]  # reocrd the every row in B_hat\n",
    "        \n",
    "        for i in range(1, N):\n",
    "            A = A.mm(self.A)\n",
    "            row_list.append(A)\n",
    "        return torch.cat(row_list,0)\n",
    "    \n",
    "    def build_B_block(self):\n",
    "        \"\"\"In MPC, express x vector in u vector and compute the new big B_hat matrix\n",
    "        [B 0 0 ...\n",
    "        [AB B 0\n",
    "        ...\n",
    "        \"\"\"\n",
    "\n",
    "        N = self.N  # number of MPC steps\n",
    "        row_list = []  # reocrd the every row in B_hat\n",
    "        \n",
    "        first_block = self.B\n",
    "        zero = Variable(torch.zeros(self.num_input, self.num_output*(N-1)))\n",
    "        zero = self.vari_gpu(zero)\n",
    "        row= torch.cat([first_block, zero],1)\n",
    "        row_list.append(row)\n",
    "        \n",
    "        for i in range(1, N):\n",
    "            first_block = self.A.mm(first_block)\n",
    "            row = torch.cat([first_block, row[:,:self.num_output*(N-1)]],1)\n",
    "            row_list.append(row)  \n",
    "            \n",
    "        return torch.cat(row_list,0)\n",
    "        \n",
    "        \n",
    "    def build_Qdiagnol_block(self, Q, P):\n",
    "        \"\"\" (num_imput*N) x (num_imput*N)\n",
    "        The last block is P for x(N)\"\"\"\n",
    "        \n",
    "        N = self.N  # number of MPC steps\n",
    "        num_input = self.num_input\n",
    "        \n",
    "        row_list = []  # reocrd the every row in B_hat\n",
    "        zero = Variable(torch.zeros(num_input, num_input*(N-1)))\n",
    "        zero = self.vari_gpu(zero)\n",
    "        row_long = torch.cat([zero, Q, zero],1)  # [0 0 ... Q 0 0 ...]\n",
    "        for i in range(N, 1, -1):\n",
    "            row_list.append(row_long[:, (i-1)*num_input : (i+N-1)*num_input])\n",
    "            \n",
    "        row = torch.cat([zero, P],1)  # last line by [0 P]\n",
    "        row_list.append(row)\n",
    "        \n",
    "        return torch.cat(row_list,0)\n",
    "    \n",
    "    def build_Rdiagnol_block(self, R):\n",
    "        \"\"\"\n",
    "        [R 0 0 ...\n",
    "        [0 R 0\n",
    "        ...\n",
    "        \"\"\"\n",
    "        N = self.N  # number of MPC steps\n",
    "        num_output = self.num_output\n",
    "        \n",
    "        row_list = []  # reocrd the every row in B_hat\n",
    "        zero = Variable(torch.zeros(num_output, num_output*(N-1)))\n",
    "        zero = self.vari_gpu(zero)\n",
    "        row_long = torch.cat([zero, R, zero],1)  # [0 0 ... Q 0 0 ...]\n",
    "        \n",
    "        for i in range(N, 0, -1):\n",
    "            row_list.append(row_long[:, (i-1)*num_output : (i+N-1)*num_output])\n",
    "        return torch.cat(row_list,0)\n",
    "        \n",
    "    def build_Q_block(self, Q, P, R, B_hat):\n",
    "        \"\"\"Build the Q_hat matrix so that MPC is tranfered into basic optimization problem\n",
    "        Q_hat = B_hat^T * diag(Q) * B_hat + diag(R)\n",
    "        \"\"\"\n",
    "        Q_diag = self.build_Qdiagnol_block(Q,P)\n",
    "        R_diag = self.build_Rdiagnol_block(R)\n",
    "        Q_hat = B_hat.t().mm(Q_diag.mm(B_hat)) + R_diag\n",
    "        return Q_hat,Q_diag \n",
    "        \n",
    "        \n",
    "    def build_G_block(self,B_hat):\n",
    "        \"\"\"Build the G matrix so that MPC is tranfered into basic optimization problem\n",
    "        G1 = [eye(num_u)]\n",
    "             [-eye(num_u)]\n",
    "        G2 = [   B_hat  ]\n",
    "             [  -B_hat  ]\n",
    "        \"\"\"\n",
    "        \n",
    "        eye = Variable(torch.eye(self.num_u))\n",
    "        eye = self.vari_gpu(eye)\n",
    "        G1 = torch.cat((eye, -eye), 0)\n",
    "        G2 = torch.cat((B_hat, -B_hat), 0)\n",
    "        # print(self.B_hat)\n",
    "        # print(G.size())\n",
    "        return G1,G2\n",
    "    \n",
    "    def vari_gpu(self, var):\n",
    "        if self.cuda:\n",
    "            var = var.cuda()\n",
    "            \n",
    "        return var\n",
    "\n",
    "    def act(self, state, epsilon, env):\n",
    "        \"\"\"The action excuted by epsilon-greedy exploration\n",
    "        \"\"\"\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(state)).unsqueeze(0) # adds extra dim when single input\n",
    "            state = self.vari_gpu(state)\n",
    "            _, u_opt = self.forward(state)\n",
    "            action = (u_opt.cpu().detach().numpy())  # compute the u*[0] \n",
    "            #print('act:q_value ',q_value)\n",
    "            #print('act:model action ',action)\n",
    "        else:\n",
    "            rand = np.random.rand(int(np.array(env.action_space.shape)))\n",
    "            high = env.action_space.high\n",
    "            low = env.action_space.low\n",
    "            action = low + rand*(high-low)\n",
    "            #print('act: ',action)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Learning(object):\n",
    "    \"\"\"Q-learning trainning. \n",
    "    \"\"\"\n",
    "    def __init__(self, config, net):\n",
    "        self.config = config\n",
    "        \n",
    "        # define current model and target model\n",
    "        num_input = config.hyperparameters[\"num_input\"]\n",
    "        num_output = config.hyperparameters[\"num_output\"]\n",
    "        num_u = config.hyperparameters[\"num_u\"]\n",
    "        self.cuda = config.use_GPU\n",
    "        device = torch.device(\"cuda:0\" if self.cuda else \"cpu\")\n",
    "        self.current_net = net(num_input=num_input, num_output=\n",
    "                           num_output, num_u=num_u, cuda=self.cuda)\n",
    "        self.current_net = self.current_net.to(device)\n",
    "        self.target_net = net(num_input=2, num_output=1, num_u=5,cuda=self.cuda).to(device)\n",
    "        self.target_net = self.target_net.to(device)\n",
    "        self.target_net.load_state_dict(self.current_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        # environments and buffer\n",
    "        buffer_size = config.hyperparameters[\"buffer_size\"]\n",
    "        self.env = config.environment\n",
    "        self.memory = ReplayMemory(buffer_size)\n",
    "        self.batch_size = config.hyperparameters[\"batch_size\"]\n",
    "        \n",
    "        # variables for training\n",
    "        self.num_episodes = config.num_episodes_to_run\n",
    "        self.target_update = config.hyperparameters[\"target_update\"]\n",
    "        self.gamma = config.hyperparameters[\"gamma\"]\n",
    "        self.loss_fun = torch.nn.MSELoss()  # Initializes the loss function\n",
    "        learning_rate = 1e-3\n",
    "        self.optimizer = torch.optim.Adam(self.current_net.parameters(),\n",
    "                                          lr=learning_rate)\n",
    "\n",
    "        self.losses = []\n",
    "        self.trac_time = []\n",
    "        self.trac_reward = []\n",
    "        self.step_total = 0  # records total steps for all trajectories\n",
    "        self.TD_error = [] # records target difference error       \n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Run the agent\"\"\"\n",
    "        for epoch in range(self.num_episodes):\n",
    "            self.run_agent_in_one_episode(epoch)\n",
    "        self.env.close() \n",
    "            \n",
    "    def run_agent_in_one_episode(self, epoch):\n",
    "        state = self.env.reset()\n",
    "        done = False  # records whether one trajectory is finished\n",
    "        t_traj = 0  # records the number of steps in one trajectory\n",
    "        r_traj = 0  # records the total rewards in one trajectory\n",
    "\n",
    "        while not done:  # Second loop: within one tractory\n",
    "\n",
    "#             position = str(env.state[0].round(decimals=2))\n",
    "#             velocity = str(env.state[1].round(decimals=2))\n",
    "            text = 'trajectory: '+str(epoch+1)\n",
    "            self.env.render(text)  # visualization of the cart position \n",
    "            epsilon = greedy_epsilon(self.step_total)\n",
    "            action = self.current_net.act(state, epsilon, self.env)\n",
    "            if action[0]>0.5 or action[0]<-0.5:  # if infeasible, start a new trajectory\n",
    "                continue\n",
    "\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "            # if done, the cart may go out of contraints, we don't \n",
    "            # want this fake reward to be saved.\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # save the step in the memory\n",
    "            # transfer into type tensor\n",
    "            state_      = self.vari_gpu(torch.FloatTensor(state)).unsqueeze(0)\n",
    "            next_state_ = self.vari_gpu(torch.FloatTensor(next_state)).unsqueeze(0)\n",
    "            action_     = self.vari_gpu(torch.FloatTensor(action)).unsqueeze(0)\n",
    "            reward_     = self.vari_gpu(torch.FloatTensor([reward])).unsqueeze(0)\n",
    "            done_       = self.vari_gpu(torch.FloatTensor([done])).unsqueeze(0)\n",
    "\n",
    "            self.memory.push(state_, action_, next_state_, reward_, done_)\n",
    "            state = next_state\n",
    "            t_traj += 1\n",
    "            r_traj += reward\n",
    "\n",
    "\n",
    "            # Third loop: train the model\n",
    "            self.step_total += 1\n",
    "            loss = 0.0\n",
    "            if self.step_total>self.batch_size:\n",
    "                for k in range(self.config.num_taining_step_every_trajectory_step):\n",
    "                    loss += self.train()\n",
    "                    self.losses.append(loss/(k+1))\n",
    "\n",
    "            # update the target network\n",
    "            if self.step_total % self.target_update == 0:  \n",
    "                self.target_net.load_state_dict(self.current_net.state_dict())\n",
    "\n",
    "            if (self.step_total)%100 == 0 and self.step_total>128:\n",
    "                print('[step_total: %d] training loss: %.3f' %\n",
    "                        (self.step_total, \n",
    "                         self.losses[self.step_total-self.batch_size-1]))\n",
    "\n",
    "            # compute the target difference error\n",
    "            try:\n",
    "                q_value, _ = self.current_net(state_,action_)  # Q(x0,u0)\n",
    "            except:\n",
    "                print(state,action)\n",
    "                print(self.current_net.para)\n",
    "                q_value, _ = self.current_net(state_,action_)\n",
    "\n",
    "            v_value, _ = self.target_net(next_state_)  # V(x1)\n",
    "            q_value = q_value.data[0,0].cpu().numpy()\n",
    "            v_value = v_value.data[0,0].cpu().numpy()\n",
    "            self.TD_error.append(reward + self.gamma * v_value * (1-done) - q_value)\n",
    "\n",
    "        #  record data of this trajectory in lists\n",
    "        self.trac_time.append(t_traj)\n",
    "        self.trac_reward.append(r_traj)\n",
    "\n",
    "       \n",
    "    def train(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        state = torch.cat(batch.state_)\n",
    "        action = torch.cat(batch.action_)\n",
    "        next_state = torch.cat(batch.next_state_)\n",
    "        reward = torch.cat(batch.reward_)\n",
    "        done = torch.cat(batch.done_)\n",
    "        \n",
    "        global Q_,p_,G1_,h1_,G2_,h2_,E_,F_,f_\n",
    "        try:\n",
    "            q_value, _ = self.current_net(state,action)  # Q(x0,u0)\n",
    "        except:\n",
    "#             print(state,action)\n",
    "#             print(self.current_net.para)\n",
    "            [Q_,p_,G1_,h1_,G2_,h2_,E_,F_,f_] = self.current_net.para\n",
    "            layer = QP_layer_e(5, 10, 20,1)\n",
    "            # solution\n",
    "            u,e, = layer ( Q_,p_,G1_,h1_,G2_,h2_,E_,F_,f_ )            \n",
    "            print(1,u,e)\n",
    "        try:\n",
    "            v_value, _ = self.target_net(next_state)  # V(x1)\n",
    "        except:\n",
    "#             print(state,action)\n",
    "#             print(self.target_net.para)\n",
    "            [Q_,p_,G1_,h1_,G2_,h2_,E_] = self.target_net.para\n",
    "            layer = QP_layer_no_eq_e(5, 10, 20)\n",
    "            # solution\n",
    "            u,e, = layer ( Q_,p_,G1_,h1_,G2_,h2_,E_ )\n",
    "            print(2,u,e)\n",
    "        # compute the expected Q values\n",
    "        expected_q_value = reward + self.gamma * v_value * (1-done)\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.loss_fun(expected_q_value, q_value) \n",
    "\n",
    "        # optimize the model  \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.data\n",
    "    \n",
    "    def vari_gpu(self, var):\n",
    "        if self.cuda:\n",
    "            var = var.cuda()\n",
    "        return var\n",
    "\n",
    "# Epsilon-greedy exploration\n",
    "# The epsilon decreases exponetially as time goes by.\n",
    "def greedy_epsilon(step_total):\n",
    "    epsilon_start = 0.8\n",
    "    epsilon_final = 0.01\n",
    "    epsilon_decay = 500\n",
    "    epsilon = epsilon_final + (epsilon_start-epsilon_final) * math.exp(-1.*step_total/epsilon_decay)\n",
    "    return epsilon    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.config import Config\n",
    "from environments.cart import MyEnv\n",
    "\n",
    "config = Config()\n",
    "config.seed = 1\n",
    "config.environment = MyEnv()\n",
    "config.num_episodes_to_run = 100\n",
    "config.num_taining_step_every_trajectory_step = 3\n",
    "config.visualise_results = False\n",
    "config.file_to_save_data_results = None\n",
    "config.file_to_save_results_graph = None\n",
    "config.use_GPU = False\n",
    "config.save_model = False\n",
    "\n",
    "\n",
    "config.hyperparameters = {\n",
    "            \"num_input\": 2,\n",
    "            \"num_output\": 1,\n",
    "            \"num_u\": 5,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"gamma\": 1.0,  # discount_rate\n",
    "            \"target_update\": 10,\n",
    "            \"batch_size\": 20,\n",
    "            \"buffer_size\": 10000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step_total: 200] training loss: 63.892\n",
      "[step_total: 300] training loss: 44.928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/season/anaconda3/lib/python3.7/site-packages/diffcp/cone_program.py:259: UserWarning: Solved/Inaccurate.\n",
      "  warnings.warn(\"Solved/Inaccurate.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step_total: 400] training loss: 639.745\n",
      "[step_total: 500] training loss: 471.927\n",
      "[step_total: 600] training loss: 1283.698\n",
      "[step_total: 700] training loss: 1228.829\n",
      "[step_total: 800] training loss: 194.959\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    net = Cvx_Nets\n",
    "    trainer = Q_Learning(config, net)\n",
    "    trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Q1,p1,G11,h11,G21,h21,E1]=[Q_.data,p_.data,\n",
    "                            G1_.data,h1_.data,\n",
    "                            G2_.data,h2_.data,E_.data ]\n",
    "layer = QP_layer_no_eq_e(5, 10, 20)\n",
    "\n",
    "# solution\n",
    "u,e, = layer ( Q1,p1,G11,h11,G21,h21,E1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.2431e-05, 1.7225e-02, 1.8794e+00, 6.9909e-01, 4.4908e+00, 5.2681e+00,\n",
       "         1.1442e+01, 1.0766e+01, 2.1602e+01, 2.1657e+01, 6.0471e-05, 1.2034e-04,\n",
       "         8.1094e-05, 1.0586e-04, 3.5383e-05, 1.2739e-04, 3.2468e-05, 7.1357e-05,\n",
       "         2.3103e-05, 6.0185e-05]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Q1,p1,G11,h11,G21,h21,E1,F1,f1]=[Q_.data,p_.data,\n",
    "                            G1_.data,h1_.data,\n",
    "                            G2_.data,h2_.data,E_.data,\n",
    "                            F_.data,f_.data ]\n",
    "layer = QP_layer_e(5, 10, 20,1)\n",
    "\n",
    "# solution\n",
    "u,e, = layer ( Q1,p1,G11,h11,G21,h21,E1,F1,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.1410e-06, 4.8009e-06, 3.0856e-05, 2.2860e-05, 7.1703e-06, 2.3710e-05,\n",
       "         3.6047e-05, 3.2273e-05, 1.9094e-04, 4.6959e-04, 7.8976e-06, 7.7903e-06,\n",
       "         3.9079e+00, 4.2183e+00, 1.3968e+01, 1.4579e+01, 3.6829e+01, 3.8065e+01,\n",
       "         8.8707e+01, 9.1371e+01]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[7.5858, 3.4945, 1.9851, 1.1377, 0.6381],\n",
       "          [3.4945, 3.4133, 1.1316, 0.6430, 0.3602],\n",
       "          [1.9851, 1.1316, 2.0752, 0.3715, 0.2027],\n",
       "          [1.1377, 0.6430, 0.3715, 1.6434, 0.1214],\n",
       "          [0.6381, 0.3602, 0.2027, 0.1214, 1.4985]]]),\n",
       " tensor([[-1955.6559, -1110.2148,  -631.8699,  -361.8481,  -202.6708]]),\n",
       " tensor([[[ 1.,  0.,  0.,  0.,  0.],\n",
       "          [ 0.,  1.,  0.,  0.,  0.],\n",
       "          [ 0.,  0.,  1.,  0.,  0.],\n",
       "          [ 0.,  0.,  0.,  1.,  0.],\n",
       "          [ 0.,  0.,  0.,  0.,  1.],\n",
       "          [-1., -0., -0., -0., -0.],\n",
       "          [-0., -1., -0., -0., -0.],\n",
       "          [-0., -0., -1., -0., -0.],\n",
       "          [-0., -0., -0., -1., -0.],\n",
       "          [-0., -0., -0., -0., -1.]]]),\n",
       " tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "          0.5000]]),\n",
       " tensor([[[-0.1074,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.6431,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.4384, -0.1074,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.3553,  0.6431,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.8237,  0.4384, -0.1074,  0.0000,  0.0000],\n",
       "          [ 0.5642,  0.3553,  0.6431,  0.0000,  0.0000],\n",
       "          [ 1.4569,  0.8237,  0.4384, -0.1074,  0.0000],\n",
       "          [ 0.9901,  0.5642,  0.3553,  0.6431,  0.0000],\n",
       "          [ 2.5702,  1.4569,  0.8237,  0.4384, -0.1074],\n",
       "          [ 1.7461,  0.9901,  0.5642,  0.3553,  0.6431],\n",
       "          [ 0.1074, -0.0000, -0.0000, -0.0000, -0.0000],\n",
       "          [-0.6431, -0.0000, -0.0000, -0.0000, -0.0000],\n",
       "          [-0.4384,  0.1074, -0.0000, -0.0000, -0.0000],\n",
       "          [-0.3553, -0.6431, -0.0000, -0.0000, -0.0000],\n",
       "          [-0.8237, -0.4384,  0.1074, -0.0000, -0.0000],\n",
       "          [-0.5642, -0.3553, -0.6431, -0.0000, -0.0000],\n",
       "          [-1.4569, -0.8237, -0.4384,  0.1074, -0.0000],\n",
       "          [-0.9901, -0.5642, -0.3553, -0.6431, -0.0000],\n",
       "          [-2.5702, -1.4569, -0.8237, -0.4384,  0.1074],\n",
       "          [-1.7461, -0.9901, -0.5642, -0.3553, -0.6431]]]),\n",
       " tensor([[  7.6217,   6.3072,  10.2540,   8.2364,  15.0208,  11.4860,  23.4390,\n",
       "           17.2058,  38.2887,  27.2941,   0.3783,   1.6928,  -2.2540,  -0.2364,\n",
       "           -7.0208,  -3.4860, -15.4390,  -9.2058, -30.2887, -19.2941]]),\n",
       " tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "           0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 1.]]])]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Q1,p1,G11,h11,G21,h21,E1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
