{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    " \n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "from utilities.buffer import ReplayMemory,Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \"\"\"Run trainning for given agent. \n",
    "    Optionally will visualise and save the results\"\"\"\n",
    "    def __init__(self, config, agent):\n",
    "        self.config = config\n",
    "        self.agent = agent\n",
    "        \n",
    "        # define current model and target model\n",
    "        num_input = config.hyperparameters[\"num_input\"]\n",
    "        num_output = config.hyperparameters[\"num_output\"]\n",
    "        num_u = config.hyperparameters[\"num_u\"]\n",
    "        self.cuda = config.use_GPU\n",
    "        device = torch.device(\"cuda:0\" if self.cuda else \"cpu\")\n",
    "        self.current_net = agent(num_input=num_input, num_output=\n",
    "                           num_output, num_u=num_u, cuda=self.cuda)\n",
    "        self.current_net = self.current_net.to(device)\n",
    "        self.target_net = agent(num_input=2, num_output=1, num_u=5,cuda=self.cuda).to(device)\n",
    "        self.current_net = self.current_net.to(device)\n",
    "        self.target_net.load_state_dict(self.current_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        # environments and buffer\n",
    "        buffer_size = config.hyperparameters[\"buffer_size\"]\n",
    "        self.env = config.environment\n",
    "        self.memory = ReplayMemory(buffer_size)\n",
    "        self.batch_size = config.hyperparameters[\"batch_size\"]\n",
    "        \n",
    "        # variables for training\n",
    "        self.num_episodes = config.num_episodes_to_run\n",
    "        self.target_update = config.hyperparameters[\"target_update\"]\n",
    "        self.gamma = config.hyperparameters[\"gamma\"]\n",
    "        self.loss_fun = torch.nn.MSELoss()  # Initializes the loss function\n",
    "        learning_rate = 1e-3\n",
    "        self.optimizer = torch.optim.Adam(self.current_net.parameters(),\n",
    "                                          lr=learning_rate)\n",
    "\n",
    "        self.losses = []\n",
    "        self.trac_time = []\n",
    "        self.trac_reward = []\n",
    "        self.step_total = 0  # records total steps for all trajectories\n",
    "        self.TD_error = [] # records target difference error       \n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Run the agent\"\"\"\n",
    "        for epoch in range(self.num_episodes):\n",
    "            self.run_agent_in_one_episode(epoch)\n",
    "        self.env.close() \n",
    "            \n",
    "    def run_agent_in_one_episode(self, epoch):\n",
    "        state = self.env.reset()\n",
    "        done = False  # records whether one trajectory is finished\n",
    "        t_traj = 0  # records the number of steps in one trajectory\n",
    "        r_traj = 0  # records the total rewards in one trajectory\n",
    "\n",
    "        while not done:  # Second loop: within one tractory\n",
    "\n",
    "#             position = str(env.state[0].round(decimals=2))\n",
    "#             velocity = str(env.state[1].round(decimals=2))\n",
    "            text = 'trajectory: '+str(epoch+1)\n",
    "            self.env.render(text)  # visualization of the cart position \n",
    "            epsilon = greedy_epsilon(self.step_total)\n",
    "            action = self.current_net.act(state, epsilon, self.env)\n",
    "\n",
    "            if action[0]>10.0 or action[0]<-10.0:  # if infeasible, start a new trajectory\n",
    "                break\n",
    "\n",
    "            next_state, reward, done, _ = self.env.step(action,t_traj)\n",
    "\n",
    "            # if done, the cart may go out of contraints, we don't \n",
    "            # want this fake reward to be saved.\n",
    "    #         if done:\n",
    "    #             break\n",
    "\n",
    "            # save the step in the memory\n",
    "            # transfer into type tensor\n",
    "            state_      = self.vari_gpu(torch.FloatTensor(state)).unsqueeze(0)\n",
    "            next_state_ = self.vari_gpu(torch.FloatTensor(next_state)).unsqueeze(0)\n",
    "            action_     = self.vari_gpu(torch.FloatTensor(action)).unsqueeze(0)\n",
    "            reward_     = self.vari_gpu(torch.FloatTensor([reward])).unsqueeze(0)\n",
    "            done_       = self.vari_gpu(torch.FloatTensor([done])).unsqueeze(0)\n",
    "\n",
    "            self.memory.push(state_, action_, next_state_, reward_, done_)\n",
    "            state = next_state\n",
    "            t_traj += 1\n",
    "            r_traj += reward\n",
    "\n",
    "\n",
    "            # Third loop: train the model\n",
    "            loss = 0.0\n",
    "            if self.step_total>self.batch_size:\n",
    "                for k in range(self.config.num_taining_step_every_trajectory_step):\n",
    "                    loss += self.train()\n",
    "                    self.losses.append(loss/(k+1))\n",
    "\n",
    "            # update the target network\n",
    "            self.step_total += 1\n",
    "            if self.step_total % self.target_update == 0:  \n",
    "                self.target_net.load_state_dict(self.current_net.state_dict())\n",
    "\n",
    "            if (self.step_total)%100 == 0 and self.step_total>128:\n",
    "                print('[step_total: %d] training loss: %.3f' %\n",
    "                                  (self.step_total, self.losses[self.step_total]))\n",
    "\n",
    "            # compute the target difference error\n",
    "            q_value, _ = self.current_net(state_,action_)  # Q(x0,u0)\n",
    "            v_value, _ = self.target_net(next_state_)  # V(x1)\n",
    "            q_value = q_value.data[0,0].cpu().numpy()\n",
    "            v_value = v_value.data[0,0].cpu().numpy()\n",
    "            self.TD_error.append(reward + self.gamma * v_value * (1-done) - q_value)\n",
    "\n",
    "        #  record data of this trajectory in lists\n",
    "        self.trac_time.append(t_traj)\n",
    "        self.trac_reward.append(r_traj)\n",
    "\n",
    "       \n",
    "    def train(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        state = torch.cat(batch.state_)\n",
    "        action = torch.cat(batch.action_)\n",
    "        next_state = torch.cat(batch.next_state_)\n",
    "        reward = torch.cat(batch.reward_)\n",
    "        done = torch.cat(batch.done_)\n",
    "\n",
    "        q_value, _ = self.current_net(state,action)  # Q(x0,u0)\n",
    "        v_value, _ = self.target_net(next_state)  # V(x1)\n",
    "\n",
    "        # compute the expected Q values\n",
    "        expected_q_value = reward + self.gamma * v_value * (1-done)\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.loss_fun(expected_q_value, q_value) \n",
    "\n",
    "        # optimize the model  \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.data\n",
    "    \n",
    "    def vari_gpu(self, var):\n",
    "        if self.cuda:\n",
    "            var = var.cuda()\n",
    "        return var\n",
    "\n",
    "# Epsilon-greedy exploration\n",
    "# The epsilon decreases exponetially as time goes by.\n",
    "def greedy_epsilon(step_total):\n",
    "    epsilon_start = 0.8\n",
    "    epsilon_final = 0.01\n",
    "    epsilon_decay = 500\n",
    "    epsilon = epsilon_final + (epsilon_start-epsilon_final) * math.exp(-1.*step_total/epsilon_decay)\n",
    "    return epsilon    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.config import Config\n",
    "from environments.cart import MyEnv\n",
    "from agents.Q_learning_off_policy import Q_learning_off_policy\n",
    "\n",
    "config = Config()\n",
    "config.seed = 1\n",
    "config.environment = MyEnv()\n",
    "config.num_episodes_to_run = 10\n",
    "config.num_taining_step_every_trajectory_step = 3\n",
    "config.visualise_results = False\n",
    "config.file_to_save_data_results = None\n",
    "config.file_to_save_results_graph = None\n",
    "config.use_GPU = False\n",
    "config.save_model = False\n",
    "\n",
    "\n",
    "config.hyperparameters = {\n",
    "            \"num_input\": 2,\n",
    "            \"num_output\": 1,\n",
    "            \"num_u\": 5,\n",
    "            \"collect\": False,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"linear_hidden_units\": [30, 15],\n",
    "            \"gamma\": 1.0,  # discount_rate\n",
    "            \"target_update\": 10,\n",
    "            \"batch_size\": 20,\n",
    "            \"buffer_size\": 10000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    AGENT = Q_learning_off_policy\n",
    "    trainer = Trainer(config, AGENT)\n",
    "    trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.env.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
