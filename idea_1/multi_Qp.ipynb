{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "from collections import deque\n",
    "import gym\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda()\n",
    "Parameter = lambda *args, **kwargs: nn.parameter.Parameter(*args, **kwargs).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Continuous_MountainCarEnv(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second': 30\n",
    "    }\n",
    "\n",
    "    def __init__(self, goal_velocity = 0):\n",
    "        self.min_action = -1.0\n",
    "        self.max_action = 1.0\n",
    "        self.min_position = -1.2\n",
    "        self.max_position = 0.6\n",
    "        self.max_speed = 0.07\n",
    "        self.goal_position = 0.45 # was 0.5 in gym, 0.45 in Arnaud de Broissia's version\n",
    "        self.goal_velocity = goal_velocity\n",
    "        self.power = 0.0015\n",
    "\n",
    "        self.low_state = np.array([self.min_position, -self.max_speed])\n",
    "        self.high_state = np.array([self.max_position, self.max_speed])\n",
    "\n",
    "        self.viewer = None\n",
    "\n",
    "        self.action_space = spaces.Box(low=self.min_action, high=self.max_action,\n",
    "                                       shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=self.low_state, high=self.high_state,\n",
    "                                            dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        position = self.state[0]\n",
    "        velocity = self.state[1]\n",
    "        force = min(max(action[0], -1.0), 1.0)\n",
    "\n",
    "        velocity += force*self.power -0.0025 * math.cos(3*position)\n",
    "        if (velocity > self.max_speed): velocity = self.max_speed\n",
    "        if (velocity < -self.max_speed): velocity = -self.max_speed\n",
    "        position += velocity\n",
    "        if (position > self.max_position): position = self.max_position\n",
    "        if (position < self.min_position): position = self.min_position\n",
    "        if (position==self.min_position and velocity<0): velocity = 0\n",
    "\n",
    "        done = bool(position >= self.goal_position and velocity >= self.goal_velocity)\n",
    "\n",
    "        reward = 0\n",
    "        if done:\n",
    "            reward = 100.0\n",
    "        reward-= math.pow(action[0],2)*0.1\n",
    "\n",
    "        self.state = np.array([position, velocity])\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.array([self.np_random.uniform(low=-0.6, high=-0.4), 0])\n",
    "        return np.array(self.state)\n",
    "\n",
    "#    def get_state(self):\n",
    "#        return self.state\n",
    "\n",
    "    def _height(self, xs):\n",
    "        return np.sin(3 * xs)*.45+.55\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.max_position - self.min_position\n",
    "        scale = screen_width/world_width\n",
    "        carwidth=40\n",
    "        carheight=20\n",
    "\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            xs = np.linspace(self.min_position, self.max_position, 100)\n",
    "            ys = self._height(xs)\n",
    "            xys = list(zip((xs-self.min_position)*scale, ys*scale))\n",
    "\n",
    "            self.track = rendering.make_polyline(xys)\n",
    "            self.track.set_linewidth(4)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            clearance = 10\n",
    "\n",
    "            l,r,t,b = -carwidth/2, carwidth/2, carheight, 0\n",
    "            car = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "            car.add_attr(rendering.Transform(translation=(0, clearance)))\n",
    "            self.cartrans = rendering.Transform()\n",
    "            car.add_attr(self.cartrans)\n",
    "            self.viewer.add_geom(car)\n",
    "            frontwheel = rendering.make_circle(carheight/2.5)\n",
    "            frontwheel.set_color(.5, .5, .5)\n",
    "            frontwheel.add_attr(rendering.Transform(translation=(carwidth/4,clearance)))\n",
    "            frontwheel.add_attr(self.cartrans)\n",
    "            self.viewer.add_geom(frontwheel)\n",
    "            backwheel = rendering.make_circle(carheight/2.5)\n",
    "            backwheel.add_attr(rendering.Transform(translation=(-carwidth/4,clearance)))\n",
    "            backwheel.add_attr(self.cartrans)\n",
    "            backwheel.set_color(.5, .5, .5)\n",
    "            self.viewer.add_geom(backwheel)\n",
    "            flagx = (self.goal_position-self.min_position)*scale\n",
    "            flagy1 = self._height(self.goal_position)*scale\n",
    "            flagy2 = flagy1 + 50\n",
    "            flagpole = rendering.Line((flagx, flagy1), (flagx, flagy2))\n",
    "            self.viewer.add_geom(flagpole)\n",
    "            flag = rendering.FilledPolygon([(flagx, flagy2), (flagx, flagy2-10), (flagx+25, flagy2-5)])\n",
    "            flag.set_color(.8,.8,0)\n",
    "            self.viewer.add_geom(flag)\n",
    "\n",
    "        pos = self.state[0]\n",
    "        self.cartrans.set_translation((pos-self.min_position)*scale, self._height(pos)*scale)\n",
    "        self.cartrans.set_rotation(math.cos(3 * pos))\n",
    "\n",
    "        return self.viewer.render(return_rgb_array = mode=='rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Able to build required model based on gym.\n",
    "    Example: CartPoleEnv\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson\n",
    "\n",
    "    Observation: \n",
    "        Type: Box(4)\n",
    "        Num\tObservation                 Min         Max\n",
    "        0\tCart Position             -4.8            4.8\n",
    "        1\tCart Velocity             -Inf            Inf\n",
    "        2\tPole Angle                 -24 deg        24 deg\n",
    "        3\tPole Velocity At Tip      -Inf            Inf\n",
    "        \n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num\tAction\n",
    "        0\tPush cart to the left\n",
    "        1\tPush cart to the right\n",
    "        \n",
    "        Note: The amount the velocity that is reduced or increased is not fixed; it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of the display)\n",
    "        Episode length is greater than 200\n",
    "        Solved Requirements\n",
    "        Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second' : 50\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5 # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = 'euler'\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\n",
    "        high = np.array([\n",
    "            self.x_threshold * 2,\n",
    "            np.finfo(np.float32).max,\n",
    "            self.theta_threshold_radians * 2,\n",
    "            np.finfo(np.float32).max])\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" Excutes one constant action to the model for one constant period.\n",
    "        The return is: np.array(self.state), reward, done, {}\n",
    "        where: \n",
    "            np.array(self.state): the next state\n",
    "            reward: r(s,a)\n",
    "            done: if the next state is the terminal state\n",
    "        \"\"\"\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "        state = self.state\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        force = self.force_mag if action==1 else -self.force_mag\n",
    "        costheta = -math.cos(theta)\n",
    "        sintheta = -math.sin(theta)\n",
    "#         temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n",
    "#         thetaacc = (self.gravity * sintheta - costheta* temp) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n",
    "#         xacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "        # approximate linear model\n",
    "#         temp = self.masspole * (1/3*self.total_mass + self.masscart) * self.length*self.length\n",
    "#         thetaacc = self.masspole*self.gravity*self.length*self.total_mass/temp * theta - self.masspole*self.length/temp * self.force_mag\n",
    "#         xacc = -self.masspole*self.masspole*self.gravity*self.length*self.length/temp * theta + 4.0/3.0*self.masspole*self.length*self.length/temp * self.force_mag\n",
    "        xacc = -theta+1*self.force_mag\n",
    "        thetaacc = theta - 1*self.force_mag\n",
    "\n",
    "        \n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x  = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else: # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x  = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "        self.state = (x,x_dot,theta,theta_dot)\n",
    "        done =  x < -self.x_threshold \\\n",
    "                or x > self.x_threshold \\\n",
    "                or theta < -self.theta_threshold_radians \\\n",
    "                or theta > self.theta_threshold_radians\n",
    "        done = bool(done)\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold*2\n",
    "        scale = screen_width/world_width\n",
    "        carty = 100 # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2\n",
    "            axleoffset =cartheight/4.0\n",
    "            cart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
    "            pole = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "            pole.set_color(.8,.6,.4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth/2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(.5,.5,.8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0,carty), (screen_width,carty))\n",
    "            self.track.set_color(0,0,0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            self._pole_geom = pole\n",
    "\n",
    "        if self.state is None: return None\n",
    "\n",
    "        # Edit the pole polygon vertex\n",
    "        pole = self._pole_geom\n",
    "        l,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
    "        pole.v = [(l,b), (l,t), (r,t), (r,b)]\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0]*scale+screen_width/2.0 # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array = mode=='rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyEnv()\n",
    "env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\" a cyclic buffer of bounded size that holds the \n",
    "    transitions observed recently. \n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-greedy exploration\n",
    "# The epsilon decreases exponetially as time goes by.\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(state).unsqueeze(0)) # adds extra dim at front\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1]\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Builds the strucre of the dnn based on QpNet for DQN.\n",
    "    \n",
    "    The struture is FC-ReLU-BN-FC-ReLU-BN-multi_QP.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_input, num_output,num_hidden, num_z=5, num_ineq=3*2*5,\n",
    "                 num_equ= 4*5,eps = 1e-4):\n",
    "        \"\"\"Inits the class.\n",
    "        num_z: (# action variable) * horizon length\n",
    "        num_ineq >= (# ineq constaints) * horizon length\n",
    "        num_equ >= (# eq constaints) * horizon length\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.normal_layers = nn.Sequential(\n",
    "            nn.Linear(num_input, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_hidden),\n",
    "            nn.Linear(num_hidden, num_z),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_z)\n",
    "        )\n",
    "        \n",
    "        # QP params for multiple QP layers\n",
    "        self.M = Variable(torch.tril(torch.ones(num_z, num_z)))\n",
    "        self.I = Variable(torch.eye(num_z))\n",
    "        self.L = []\n",
    "        self.G = []\n",
    "        self.A = []\n",
    "        self.z0 = []\n",
    "        self.s0 = []\n",
    "        for i in range(2):\n",
    "            self.L.append(Parameter(torch.tril(torch.rand(num_z, num_z))))\n",
    "            self.G.append(Parameter(torch.Tensor(num_ineq,num_z).uniform_(-1,1)))\n",
    "            self.A.append(Parameter(torch.Tensor(num_equ,num_z).uniform_(-1,1)))\n",
    "            self.z0.append(Parameter(torch.zeros(num_z)))\n",
    "            self.s0.append(Parameter(torch.ones(num_ineq)))\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \"\"\"Builds the forward strucre of the QPNet.\n",
    "        \"\"\"\n",
    "        #print('f,input',x.size())\n",
    "        x = self.normal_layers(x)\n",
    "        #print('f,nz',x.size())\n",
    "        # Set up the qp parameters Q=L*L^T+ϵ*I, h = G*z_0+s_0, b = A*z0.\n",
    "        Qp_list = []\n",
    "        for i in range(2):\n",
    "            L = self.M*self.L[i]\n",
    "            Q = L.mm(L.t()) + self.eps*self.I\n",
    "            h = self.G[i].mv(self.z0[i])+self.s0[i]\n",
    "            b = self.A[i].mv(self.z0[i])\n",
    "            z_opt = QPFunction(verbose=-1)(Q, x, self.G[i], h, self.A[i], b)\n",
    "            Qp_value = (z_opt.mm(Q)*z_opt/2 + x*z_opt).sum(1).unsqueeze(1)\n",
    "            #print('forward: Qp_value',Qp_value.size())\n",
    "            # 1/2*z^T*Q*z + p^T*z\n",
    "            Qp_list.append(Qp_value)\n",
    "        x = torch.cat(Qp_list,1)\n",
    "        #print('forward:Qp_2action',x.size())\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        \"\"\"The action excuted by epsilon-greedy exploration\n",
    "        \"\"\"\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(state).unsqueeze(0)) # adds extra dim at front\n",
    "            q_value = self.forward(state)\n",
    "            #print('act:q_value ',q_value)\n",
    "            action  = q_value.max(1)[1]\n",
    "            #print('act:model action ',action)\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randrange(env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyEnv()\n",
    "model = DQN(4, env.action_space.n,20)\n",
    "\n",
    "model = model.cuda()\n",
    "model.eval()    \n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "state1      = Variable(torch.FloatTensor(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(state)\n",
    "state1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(replay_buffer,batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    print(state.size)\n",
    "    state      = Variable(torch.FloatTensor(state))\n",
    "    print(state.size)\n",
    "    next_state = Variable(torch.FloatTensor(next_state))\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    q_values      = model(state)\n",
    "    next_q_values = model(next_state)\n",
    "    #print('loss:q_values',q_values.size())\n",
    "    #print('loss:action', action.size())\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "<built-in method size of Tensor object at 0x7f4117fd5798>\n"
     ]
    }
   ],
   "source": [
    "num_run_period = []  # records the # periods in one trajectory\n",
    "t = 0\n",
    "num_frames = 33\n",
    "batch_size = 32\n",
    "replay_buffer = ReplayBuffer(2 * batch_size)  #2 possible actions each round, batch_size rounds per batch\n",
    "gamma      = 0.99\n",
    "losses = []\n",
    "episode_reward = 0\n",
    "avg_loss = 0\n",
    "avg_losses = []\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    t = t+1\n",
    "    env.render()\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "    next_state, reward, done, _ = env.step(int(action))\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    #env.render()\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        num_run_period.append(t)\n",
    "        t = 0\n",
    "    \n",
    "    if len(replay_buffer) > batch_size:\n",
    "        loss = compute_td_loss(replay_buffer,batch_size)\n",
    "        losses.append(loss.data.item())\n",
    "        avg_loss = avg_loss + loss.data.item()\n",
    "        avg_losses.append(avg_loss/frame_idx)\n",
    "#     try:\n",
    "#         avg_loss = avg_loss + loss.data.item()\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "    if frame_idx % 50 == 0:\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(131)\n",
    "        plt.plot(losses)\n",
    "        plt.subplot(132)\n",
    "        plt.plot(avg_losses)\n",
    "        plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 11,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 8]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_run_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1],\n",
       "        [3, 1]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([2,3]).unsqueeze(1)\n",
    "b = torch.tensor([1,1]).unsqueeze(1)\n",
    "c = torch.cat([a,b],1)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
